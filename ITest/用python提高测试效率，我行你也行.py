#coding:utf-8
import time

'''用csv文件给jmeter做参数化时碰到了一个场景：将研发给我的5万条数据依照数量平均分成五份，再分别用jmeter跑一次流程。

按照我以往的工作方式，我大概会新建好5个文件，然后分别给这5个文件依次拷贝1万条数据。这样做既费精力又伤眼睛，相信很多人都有这种体会。庆幸的是吴老师上一节课正好讲过用python读写文件的方法，我这次正好用上，这里做一个小分享。

首先，理一下完成目标的思路：从源文件中读取数据写入另一个文件中，每个文件满1万行，就换一个文件。这里我想到的两种实现方式：

1）读取数据存到列表中，将列表中数据存到文件中，每写入1万条数据便换一个文件

2）每读出一条数据便写入到新的文件中，每个文件满1万条数据就换一个文件写。

'''

# 接着，开始编写代码：

# 1）读一条写一条:
startTime = time.time()#程序开始时间
with open(r'E:\工作\1010同时做任务\源数据.csv', 'r') as fp:
    for i in range(5):
        count = 0   #给文件中写入数据计数，每换一个文件就将数据量重置为0
        while count < 10000:   #每个文件写入1万条数据就写入另一个文件
            data = fp.readline().strip()   #按行读取数据时会有换行符，用strip()方法去掉
            with open(r'E:\工作\1010同时做任务\测试数据'+str(i + 1)+'.csv', 'a+') as fq:   #目标路径不存在时，则自动创建文件。
                fq.write(data + '\n')   #将数据写入文件时，默认没有换行符，需要在每条数据后面加上“\n”
            count+=1   #给每个文件写入的数据计数，超过1万条跳出循环往下一个文件写入
            print ('已创建第%s个文件' % str(i+1))
# 程序运行结束时间
endTime = time.time()
print ('共花费%秒' % str(int(endTime-startTime)))

# 2）每读出一条数据便写入到新的文件中，每个文件满1万条数据就换一个文件写：
startTime = time.time()
with open(r'E:\工作\1010同时做任务\源数据.csv', 'r') as fp:
    data = []   #定义一个列表用来装数据
    while 1:
        data.append(fp.readline().strip())
        if len(data) > 50000:   #读取5万条数据
            break
for i in range(5):  #将5万条数据分5次写入到不同文件中
    with open(r'E:\工作\1010同时做任务\测试数据'+str(i+6)+'.csv', 'w') as fp:
        startNum = i*10000
        for j in data[startNum:startNum+10000:]:    #每次从列表中取一万条数据写入文件
            fp.write(j+'\n')
    print ('已创建第%s个文件' % str(i+1))
endTime = time.time()
print ('共花费%s秒' % str(int(endTime-startTime)))


'''由结果图可以看出：第一种方式是牺牲时间换空间的方式，运行时占用内存很小，但是弊端在于需要频繁打开文件写入数据，速度较慢；
第二种方式速度很快，在电脑内存资源吃紧的时候可以减少每次中转的数据量。
本人电脑内存远超这些数据大小，此场景选择第二种方式，大家可根据需求选择不同的读写方式。


结束语：
很简单的几行代码，写完加调试，一共也就几分钟，回过头来看，这几行代码任然有改进的空间，比如对源文件每一行的数据进行校验。
这是本人第一次在工作中主动用编程来实现自己的想法，实际上也是节约了不少的精力和时间。
相信随着学习深入，我会写更多的小脚本来提升自己或其他人的工作效率。'''